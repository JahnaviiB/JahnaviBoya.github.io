<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>BART</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="6f6c2bcf-ae15-4638-8ff3-5392385c67b7" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">📄</span></div><h1 class="page-title">BART</h1></header><div class="page-body"><h2 id="b6ecfc97-fb48-4f02-8718-4d4b43ae7c4d" class="">Official</h2><figure id="a5a726d9-7f96-4693-83f7-77af18c1875a"><a href="https://github.com/facebookresearch/GENRE" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">GitHub - facebookresearch/GENRE: Autoregressive Entity Retrieval</div><div class="bookmark-description">The GENRE (Generative ENtity REtrieval) system as presented in Autoregressive Entity Retrieval implemented in pytorch. The mGENRE system as presented in Multilingual Autoregressive Entity Linking Please consider citing our works if you use code from this repository. In a nutshell, (m)GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned BART architecture or mBART (for multilingual).</div></div><div class="bookmark-href"><img src="https://github.com/favicon.ico" class="icon bookmark-icon"/>https://github.com/facebookresearch/GENRE</div></div><img src="https://opengraph.githubassets.com/1af11cbb46bb490d6aecefcaa503fc02d4641f8433a4e45d0653d938f86adce5/facebookresearch/GENRE" class="bookmark-image"/></a></figure><h2 id="671aa121-c6b1-4288-bf49-750b37efbdfc" class="">Summary</h2><p id="dfdbce8b-3c74-4133-91fb-ca8235746935" class="">BART is a denoising autoencoder for pre-training sequence-to-sequence models. BART is trained by corrupting text with an arbitrary noise function and learning a model to reconstruct the original text.</p><p id="e62fd2b5-28d5-4b46-bc93-4c272bee8841" class="">It uses a standard Transformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pre-training schemes.</p><p id="ede4c39e-3a4e-4d15-a66a-a3dab7a42d88" class="">BART also presents a new scheme for machine translation where a BART model is stacked above a few additional transformer layers. These layers are trained to essentially translate the foreign language to noised English, by propagation through BART, thereby using BART as a pre-trained target-side language model.</p><h2 id="0edb6fc8-22a6-4680-8099-f95802da726d" class="">Architecture</h2><figure id="35d59202-6697-4af0-b168-08fabb3794d9" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_095123.jpg"><img style="width:1631px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_095123.jpg"/></a></figure><p id="e0e21405-7349-4fcd-97c0-4af9dff92271" class="">It uses standard sequence-to-sequence Transformer architecture except, following GPT, they modify ReLU activation functions to GeLUs and initialise parameters from N(0,0.02). The architecture is closely related to BERT with the following differences:</p><ul id="b7a022b1-3702-4b6c-a76b-2a72363b4c63" class="bulleted-list"><li style="list-style-type:disc">each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model)</li></ul><ul id="cb5287dd-c8ea-4dfc-9790-53796520aec6" class="bulleted-list"><li style="list-style-type:disc">BERT uses an additional feed-forward network before word prediction while BART does not</li></ul><h2 id="2acbfcd9-709e-4bde-b542-b0f9fb052ab6" class="">Pre-Training Task</h2><p id="03579580-e695-4546-bf04-0a0511bbcf33" class="">BART pre-trains a model combining Bidirectional and Auto-Regressive Transformers. Pre-training has two stages:</p><ul id="95d42e0a-0e6a-4631-a0ac-e6c4e9dfef32" class="bulleted-list"><li style="list-style-type:disc">text is corrupted with an arbitrary noising function</li></ul><ul id="b5d77970-c5fb-4fcd-8345-0cd1b7439428" class="bulleted-list"><li style="list-style-type:disc">sequence-to-sequence model is learned to reconstruct the original text</li></ul><p id="f6c5cd7f-4f44-4b2c-8b76-011f52c86ff2" class="">BART optimizes a reconstruction loss - the cross-entropy between the decoder’s output and the original document. BART allows us to apply any type of document corruption.</p><figure id="e4c93f50-c164-4aed-a056-fafbac379e47" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_110225.jpg"><img style="width:2091px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_110225.jpg"/></a></figure><h2 id="5936cd5d-1153-4342-ac34-e13e2bd683e2" class="">Experiments</h2><figure id="1e7315b1-6bfb-4594-8b75-2c087147468d" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112735.jpg"><img style="width:1650px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112735.jpg"/></a></figure><figure id="912f28f7-7d65-4284-a8d1-450a8918c1c9" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113839.jpg"><img style="width:1051px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113839.jpg"/></a></figure><h2 id="2adda7af-92fd-4a3e-8cf5-e0ab5a8176e1" class="">Performance</h2><p id="a73518fc-0963-4c0e-abb1-aac2b230c5ba" class="">We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme where spans of text are replaced with a single mask token.</p><figure id="d4f18a38-1055-4f1c-ba8f-21ca3a0a4d4d" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112905.jpg"><img style="width:2123px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112905.jpg"/></a></figure><figure id="9b0996c8-9ecb-41f3-bf75-37386710748e" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113425.jpg"><img style="width:1739px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113425.jpg"/></a></figure><h2 id="3f4f6da8-19a5-4385-9c62-966fb2dec283" class="">Fine-Tuning</h2><p id="4040066f-566e-41ed-9d38-6caf4093271c" class="">The representations produced can be used in several ways for downstream applications such as sequence classification, token classification, sequence generation and machine translation.</p><figure id="d1865d7c-b9db-4765-94dc-cfb8efd754f8" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112209.jpg"><img style="width:2154px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112209.jpg"/></a></figure><figure id="5761d366-5e72-44ad-a265-ee913693e1b1" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112817.jpg"><img style="width:2119px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112817.jpg"/></a></figure><figure id="aa378584-ce1b-41c8-8ad8-1d378d71b38f" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112950.jpg"><img style="width:1797px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_112950.jpg"/></a></figure><figure id="fed03e45-dbbd-4e59-90b5-e5431c7e6f6d" class="image"><a href="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113342.jpg"><img style="width:1556px" src="BART%206f6c2bcfae1546388ff35392385c67b7/IMG_20221207_113342.jpg"/></a></figure><h2 id="260e40e0-ce7d-4489-95f0-58edec2deacd" class="">Further Readings</h2><figure id="7b956a02-06d7-441c-9364-5a8b5234f6f6"><a href="https://techblog.geekyants.com/text-summarization-using-facebook-bart-large-cnn" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Text Summarization using Facebook BART Large CNN</div><div class="bookmark-description">Text Summarization using Facebook BART Large CNN text summarization is a natural language processing (NLP) technique that enables users to quickly and accurately summarize vast amounts of text without losing the crux of the topic. We&#x27;ve all read articles and other lengthy writings that completely divert our attention from the topic at hand because of a tonne of extraneous information.</div></div><div class="bookmark-href"><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1607065146253/dfasRq7NV.png?auto=compress,format&amp;format=webp&amp;fm=png" class="icon bookmark-icon"/>https://techblog.geekyants.com/text-summarization-using-facebook-bart-large-cnn</div></div><img src="https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1660916998886%2F7DCJ8iBae.jpg%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng" class="bookmark-image"/></a></figure><figure id="ea46966e-0013-456f-b18c-57af08a263cd"><a href="https://dair.ai/BART-Summary/" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">BART: Are all pretraining techniques created equal?</div><div class="bookmark-description">Paper summary: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension , Oct. 2019. ( link) In this paper, Lewis et al. present valuable comparative work on different pre-training techniques and show how this kind of work can be used to guide large pre-training experiments reaching state-of-the-art (SOTA) results.</div></div><div class="bookmark-href"><img src="https://dair.ai/favicon.png" class="icon bookmark-icon"/>https://dair.ai/BART-Summary/</div></div><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FAntonioLprd%2FMp23o_Xx8j.jpg?alt=media&amp;token=72dbb3c1-93a5-4b9d-8cec-3e006952568e" class="bookmark-image"/></a></figure><figure id="c87ec0b2-a44a-4a36-9cdd-482af505f462"><div class="source"><a href="https://www.youtube.com/watch?v=MxNnl_gHV1Y">https://www.youtube.com/watch?v=MxNnl_gHV1Y</a></div></figure><figure id="870bca29-5391-4530-9938-5531f5bbb131"><a href="https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">paper summary: &quot;BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation...</div><div class="bookmark-description">While the model architecture is quite simple, the key contribution of this work is an elaborate experimentation on the various pretraining tasks. While many other papers were about &quot;oh we used this pretraining task along with others and got better performance! WOW&quot;, this paper is more about &quot;from all those many pretraining tasks, which are really helpful and effective?&quot;</div></div><div class="bookmark-href"><img src="https://miro.medium.com/1*m-R_BkNf1Qjr1YbyOIJY2w.png" class="icon bookmark-icon"/>https://medium.com/mlearning-ai/paper-summary-bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-69e41dfbb7fe</div></div><img src="https://miro.medium.com/max/532/1*CaRevRvgejf4TEJVBDBK_A.png" class="bookmark-image"/></a></figure><figure id="db1b40c5-b950-407f-b6c1-81c7f45c034b"><a href="https://www.projectpro.io/article/transformers-bart-model-explained/553" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Transformers BART Model Explained for Text Summarization</div><div class="bookmark-description">Generalizing BERT (due to the bidirectional encoder) and GPT2 (with the left to right decoder) - Enter the World of a Mysterious new Seq2Seq Model - BART Models Abstractive Text Summarization using Transformers-BART Model Downloadable solution code | Explanatory videos | Tech Support Start Project HuggingFace Transformer models provide an easy-to-use implementation of some of the best performing models in natural language processing.</div></div><div class="bookmark-href"><img src="https://daxg39y63pxwu.cloudfront.net/dezyre/images/favicon_io/favicon-16x16.png" class="icon bookmark-icon"/>https://www.projectpro.io/article/transformers-bart-model-explained/553</div></div><img src="https://daxg39y63pxwu.cloudfront.net/images/blog/transformers-bart-model-explained/image_55116624341642833003956.png" class="bookmark-image"/></a></figure><figure id="0a2b5b91-295c-4487-b486-b8a875b3d348"><a href="https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Introducing BART</div><div class="bookmark-description">For the past few weeks, I worked on integrating BART into transformers. This post covers the high-level differences between BART and its predecessors and how to use the new BartForConditionalGeneration to summarize documents. Leave a comment below if you have any questions! In October 2019, teams from Google and Facebook published new transformer papers: T5 and BART.</div></div><div class="bookmark-href"><img src="https://sshleifer.github.io/blog_v2/images/favicon.ico" class="icon bookmark-icon"/>https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html</div></div><img src="https://sshleifer.github.io/blog_v2/images/text_infilling.png" class="bookmark-image"/></a></figure><p id="657d119e-993e-4314-b401-d86c7ee0b1a6" class="">
</p></div></article></body></html>