<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Multi-View S2S</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="e3e9d9d4-f5e4-4d8b-9853-a8ce550144f8" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">📄</span></div><h1 class="page-title">Multi-View S2S</h1></header><div class="page-body"><h2 id="f733f1ae-b20e-4985-b0f3-e6198b25e102" class="">Official</h2><figure id="c0690033-3b1a-4cd3-b855-0805425bde68"><a href="https://github.com/SALT-NLP/Multi-View-Seq2Seq" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">GitHub - SALT-NLP/Multi-View-Seq2Seq: Source codes for the paper &quot;Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization&quot;</div><div class="bookmark-description">This repo contains codes for the following paper: Jiaao Chen, Diyi Yang: Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization, EMNLP 2020 If you would like to refer to it, please cite the paper mentioned above. These instructions will get you running the codes of Multi-View Conversation Summarization.</div></div><div class="bookmark-href"><img src="https://github.com/favicon.ico" class="icon bookmark-icon"/>https://github.com/SALT-NLP/Multi-View-Seq2Seq</div></div><img src="https://opengraph.githubassets.com/ddc98b1c6d81308cbda177d373e4b4577d3ed79345f5f47f987cb5ce4ebffd3b/SALT-NLP/Multi-View-Seq2Seq" class="bookmark-image"/></a></figure><h2 id="d959e54d-24b3-426d-8364-d5f4d98c0bc0" class="">Summary</h2><p id="01418075-5c84-41f2-bbf9-71e73ecfffce" class="">We propose a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries.</p><figure id="7ccfa15a-1ac3-4075-908d-6ae75defd1fc" class="image"><a href="Screenshot_from_2022-12-09_10-58-02.png"><img style="width:1504px" src="Screenshot_from_2022-12-09_10-58-02.png"/></a></figure><ul id="37cffdaa-9d4c-4398-b2b5-9d4831850a50" class="bulleted-list"><li style="list-style-type:disc">we propose to utilize rich conversational structures, id est, structured views and the generic views for abstraction conversation summarization</li></ul><ul id="f1f372ba-9cea-463f-9591-36f8defd1d78" class="bulleted-list"><li style="list-style-type:disc">we design a multi-view sequence-to-sequnce model that consists of a conversation encoder to encode different views and a multi-view decoder with multi-view attention to generate dialogue summaries</li></ul><ul id="25087c12-5e4e-48f5-af8d-6c953497ce27" class="bulleted-list"><li style="list-style-type:disc">we perform experiments on a large-scale conversation summarization dataset (SAMSum) and demonstrate the effectiveness of our proposed methods</li></ul><ul id="f0ccc412-9506-4da7-8e7f-484775f51d80" class="bulleted-list"><li style="list-style-type:disc">we conduct thorough error analyses and discuss specific challenges that current approaches faced with this task</li></ul><h2 id="ff540eea-938b-4951-84ca-a3d47cc32188" class="">Architecture</h2><figure id="c7fd10f2-7b88-4272-87ba-5504d6beed1f" class="image"><a href="Screenshot_from_2022-12-09_11-04-07.png"><img style="width:1546px" src="Screenshot_from_2022-12-09_11-04-07.png"/></a></figure><figure id="633faab5-fb6c-4df6-8ac8-7361b98972c7" class="image"><a href="Screenshot_from_2022-12-09_11-15-57.png"><img style="width:736px" src="Screenshot_from_2022-12-09_11-15-57.png"/></a></figure><h2 id="f667c4a2-5f27-455a-aaa0-8386501b5018" class="">Pre-Training</h2><figure id="5f33bab7-3d19-471c-b8be-b83ba6254d6c" class="image"><a href="Screenshot_from_2022-12-09_11-07-21.png"><img style="width:768px" src="Screenshot_from_2022-12-09_11-07-21.png"/></a></figure><figure id="0196bef0-76ce-4be6-b497-d5d112e85ab1" class="image"><a href="Screenshot_from_2022-12-09_11-07-40.png"><img style="width:773px" src="Screenshot_from_2022-12-09_11-07-40.png"/></a></figure><figure id="7f714707-b27b-4379-88d6-e7b71935292e"><a href="https://github.com/UKPLab/sentence-transformers" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">GitHub - UKPLab/sentence-transformers: Multilingual Sentence &amp; Image Embeddings with BERT</div><div class="bookmark-description">This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.</div></div><div class="bookmark-href"><img src="https://github.com/favicon.ico" class="icon bookmark-icon"/>https://github.com/UKPLab/sentence-transformers</div></div><img src="https://opengraph.githubassets.com/fe37efc97e894c7b219f883e703252e93b71d74694df73364e403d7455260e9b/UKPLab/sentence-transformers" class="bookmark-image"/></a></figure><h2 id="aabf0ec0-0c27-40d4-b191-c6775d04ec23" class="">Experiments</h2><figure id="74987dd8-0cda-400d-ae2f-085066df914b" class="image"><a href="Screenshot_from_2022-12-09_11-09-09.png"><img style="width:1433px" src="Screenshot_from_2022-12-09_11-09-09.png"/></a></figure><figure id="d4541da6-49f8-47b5-99eb-473eddd77aa2"><a href="https://github.com/pltrdy/rouge" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">GitHub - pltrdy/rouge: A full Python Implementation of the ROUGE Metric (not a wrapper)</div><div class="bookmark-description">This implementation is independant from the &quot;official&quot; ROUGE script (aka. ROUGE-155). Results may be slighlty different, see discussions in #2. Clone &amp; Install git clone https://github.com/pltrdy/rouge cd rouge python setup.py install pip install -U .</div></div><div class="bookmark-href"><img src="https://github.com/favicon.ico" class="icon bookmark-icon"/>https://github.com/pltrdy/rouge</div></div><img src="https://opengraph.githubassets.com/a99400eb6cdf04d00c5a6b1f8eddab4a94330fda7bbad471085ca3cc959c9672/pltrdy/rouge" class="bookmark-image"/></a></figure><figure id="bf818b5f-b66b-4d6c-8fbd-fbf481db6852" class="image"><a href="Screenshot_from_2022-12-09_11-09-24.png"><img style="width:1496px" src="Screenshot_from_2022-12-09_11-09-24.png"/></a></figure><h2 id="128a47d8-d3d0-4b8d-9ed0-b5a78aa9fbfa" class="">Performance</h2><figure id="2ffbb361-264d-44ea-b20b-c8d6a9042a1a" class="image"><a href="Screenshot_from_2022-12-09_11-09-59.png"><img style="width:734px" src="Screenshot_from_2022-12-09_11-09-59.png"/></a></figure><figure id="cd203ef4-b735-4217-a434-247d7037ef07"><a href="https://www.mturk.com/" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Amazon Mechanical Turk</div><div class="bookmark-description">Access a global, on-demand, 24x7 workforce Amazon Sagemaker Ground Truth Plus: Fully managed data labeling service Ground Truth Plus is a turnkey data labeling service that enables you to easily create high-quality training datasets without having to build labeling applications or manage the labeling workforce on your own.</div></div><div class="bookmark-href"><img src="https://www.mturk.com/assets/images/favicon.ico" class="icon bookmark-icon"/>https://www.mturk.com/</div></div><img src="https://www.mturk.com/assets/images/usfoods.png" class="bookmark-image"/></a></figure><figure id="8115af68-5b8f-4de6-9f29-5480da7bd78f" class="image"><a href="Screenshot_from_2022-12-09_11-13-30.png"><img style="width:745px" src="Screenshot_from_2022-12-09_11-13-30.png"/></a></figure><figure id="6a6ef56e-0e05-4250-b16e-621877d40a8a" class="image"><a href="Screenshot_from_2022-12-09_11-14-06.png"><img style="width:761px" src="Screenshot_from_2022-12-09_11-14-06.png"/></a></figure><figure id="f04c44bb-5955-4e3b-8c3a-1696b12db6a3" class="image"><a href="Screenshot_from_2022-12-09_11-15-01.png"><img style="width:762px" src="Screenshot_from_2022-12-09_11-15-01.png"/></a></figure><figure id="c5d5a50e-8f8f-475a-9e36-65cff8b733e4" class="image"><a href="Screenshot_from_2022-12-09_11-15-22.png"><img style="width:757px" src="Screenshot_from_2022-12-09_11-15-22.png"/></a></figure><figure id="1893924b-75b4-4cee-b7a5-083d043575a0" class="image"><a href="Screenshot_from_2022-12-09_11-17-10.png"><img style="width:1516px" src="Screenshot_from_2022-12-09_11-17-10.png"/></a></figure><figure id="f28bfc4b-948a-438c-8bfd-2fb8adfe2258" class="image"><a href="Screenshot_from_2022-12-09_11-17-21.png"><img style="width:1465px" src="Screenshot_from_2022-12-09_11-17-21.png"/></a></figure><h2 id="cd6a7e27-ccc2-4b74-b48a-ef5b043978e5" class="">Further Readings</h2><figure id="134ba1a1-c7a9-45e1-b1a0-ee99c02a36cd"><a href="https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Natural Language Understanding with Sequence to Sequence Models</div><div class="bookmark-description">Natural Language Understanding (NLU), the technology behind conversational AI (chatbots, virtual assistant, augmented analytics) typically includes the intent classification and slot filling tasks, aiming to provide a semantic tool for user utterances. Intent classification focuses on predicting the intent of the query, while slot filling extracts semantic concepts in the query.</div></div><div class="bookmark-href"><img src="https://miro.medium.com/fit/c/256/256/1*VzTUkfeGymHP4Bvav-T-lA.png" class="icon bookmark-icon"/>https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b</div></div><img src="https://miro.medium.com/max/600/0*serGv8BnrYb5g5Z0.jpg" class="bookmark-image"/></a></figure><figure id="723b5968-c4d4-45d1-b893-b7ffbd177656"><a href="https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/" class="bookmark source"><div class="bookmark-info"><div class="bookmark-text"><div class="bookmark-title">Seq2Seq Model | Sequence To Sequence With Attention</div><div class="bookmark-description">Deep Learning at scale is disrupting many industries by creating chatbots and bots never seen before. On the other hand, a person just starting out on Deep Learning would read about Basics of Neural Networks and its various architectures like CNN and RNN.</div></div><div class="bookmark-href"><img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/02/logo_square_v2.jpg" class="icon bookmark-icon"/>https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/</div></div><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/flight-1179587_1920.jpg" class="bookmark-image"/></a></figure><p id="180ccd43-1eab-47b5-b39c-4f5cff87a736" class="">
</p></div></article></body></html>